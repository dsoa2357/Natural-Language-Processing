{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17장_자연어처리.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaJSVLMykNIUGsfxMFf38+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdiKcNRbtc5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6340915a-739d-46c2-f050-68f6e7bcdbdb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_1_DL3M1YLz",
        "colab_type": "text"
      },
      "source": [
        "#### 주어진 문장을 '단어'로 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX9kdlrHteb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텍스트 전처리 함수 text_to_word_sequence(), Tokenizer() 호출\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjKYTpRh1k9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "76e21b71-206e-4b14-c939-929cfbc454f7"
      },
      "source": [
        "# 전처리할 텍스트 정하기\n",
        "text = '해 보지 않으면 해낼 수 없다'\n",
        "\n",
        "# 해당 텍스트 토큰화\n",
        "result = text_to_word_sequence(text)\n",
        "print('\\n원문:\\n',text)\n",
        "print('\\n토큰화:\\n',result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "원문:\n",
            " 해 보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해', '보지', '않으면', '해낼', '수', '없다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liNS-QEK33wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전처리하려는 세 개의 문장 정하기\n",
        "\n",
        "docs={'먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "      '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "      '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3aDBlsu4WcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "44ca2459-8140-4fd2-fbc1-901f79585d5c"
      },
      "source": [
        "# 토큰화 함수를 이용해 전처리하는 과정\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "\n",
        "# 각 옵션에 맞춰 단어의 빈도 수를 계산한 결과 출력\n",
        "print('\\n단어 카운트:\\n', token.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('토큰화한', 1), ('결과는', 1), ('딥러닝에서', 2), ('사용할', 1), ('수', 1), ('있습니다', 1), ('텍스트의', 2), ('단어로', 1), ('토큰화해야', 1), ('인식됩니다', 1), ('먼저', 1), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Q4CHg55tF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "efdd3ec3-a55c-43bd-9638-2226a78713ec"
      },
      "source": [
        "# 출력되는 순서는 랜덤\n",
        "print('\\n문장 카운트:\\n', token.document_count)\n",
        "print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n",
        "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "문장 카운트:\n",
            " 3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'딥러닝에서': 2, '사용할': 1, '토큰화한': 1, '수': 1, '결과는': 1, '있습니다': 1, '텍스트의': 2, '단어로': 1, '인식됩니다': 1, '토큰화해야': 1, '먼저': 1, '토큰화합니다': 1, '각': 1, '단어를': 1, '나누어': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'딥러닝에서': 1, '텍스트의': 2, '토큰화한': 3, '결과는': 4, '사용할': 5, '수': 6, '있습니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '먼저': 11, '각': 12, '단어를': 13, '나누어': 14, '토큰화합니다': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S8cWnLh6NVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUdw5VEpQrvz",
        "colab_type": "text"
      },
      "source": [
        "#### 영화 리뷰가 긍정적인지 부정적인지 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZ3wOiVQudY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy import array                                           # 배열화\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer         # 토큰화\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # 단어 임베딩\n",
        "from tensorflow.keras.models import Sequential                    # 모델 설정\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTHn3X23SlO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텍스트 리뷰 자료 지정\n",
        "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\n",
        "        \"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBiTMF5zS1Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스 지정\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwyj1zlaS7rK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c9d9c4d8-b8bf-4878-f7c0-2ddfc4edb14a"
      },
      "source": [
        "# 단어 토큰화 \n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwyO4GBCTaPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8e530c36-2143-4a7c-ce5b-0a5f32814c60"
      },
      "source": [
        "x = token.texts_to_sequences(docs)\n",
        "print(x) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPo2AIy1TTUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "d58c934d-eaa8-4cba-949e-7dd6e8e9f9f6"
      },
      "source": [
        "# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n",
        "padded_x = pad_sequences(x, 4)  \n",
        "print(\"\\n패딩 결과:\\n\", padded_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "패딩 결과:\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAb4KMGT4QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#임베딩에 입력될 단어 수 지정\n",
        "word_size = len(token.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5ptE36nUFjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "2bc7ad0b-97f4-459f-baa1-11d708c6f9b6"
      },
      "source": [
        " #단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과 출력\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size, 8, input_length=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(padded_x, classes, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.7000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.8000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.8000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6735 - accuracy: 0.9000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.9000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.9000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.9000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6652 - accuracy: 0.9000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6631 - accuracy: 0.9000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.9000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.9000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.9000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6547 - accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6526 - accuracy: 0.9000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.9000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6dce647d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RqimQWEUj4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "779a78eb-015d-4225-8f8a-39900c55089d"
      },
      "source": [
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.9000\n",
            "\n",
            " Accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}